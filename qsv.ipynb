{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14503a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using dataset: /home/sugeethjsa/Documents/GitHub/dhivyanj/qff25-hthn/weatherHistory.csv\n",
      "HYBRID QUANTUM-CLASSICAL RENEWABLE ENERGY CLASSIFIER (demo)\n",
      "✅ Using local dataset: /home/sugeethjsa/Documents/GitHub/dhivyanj/qff25-hthn/weatherHistory.csv\n",
      "Loaded /home/sugeethjsa/Documents/GitHub/dhivyanj/qff25-hthn/weatherHistory.csv with shape (96453, 12)\n",
      "Warning: could not find column for solar_irradiance, generating synthetic values\n",
      "Warning: could not find column for wind_speed, generating synthetic values\n",
      "Loaded 365 days; unique labels: ['Geothermal' 'Mixed' 'Solar' 'Wind']\n",
      "Running QRC on 365 days... (this may be slow with many qubits/layers)\n",
      "  Processed 50/365 days\n",
      "  Processed 100/365 days\n",
      "  Processed 150/365 days\n",
      "  Processed 200/365 days\n",
      "  Processed 250/365 days\n",
      "  Processed 300/365 days\n",
      "  Processed 350/365 days\n",
      "Reservoir states shape: (365, 5)\n",
      "Saved example GASF image as gasf_example_day1.png\n",
      "Label mapping: {'Geothermal': 0, 'Mixed': 1, 'Solar': 2, 'Wind': 3}\n",
      "Training QSVC: 292 train samples, 73 test samples\n",
      "Computing training kernel matrix (may take time)...\n",
      "Computing test kernel matrix...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.53      0.57      0.55        30\n",
      "           3       0.54      0.67      0.59        33\n",
      "\n",
      "    accuracy                           0.53        73\n",
      "   macro avg       0.27      0.31      0.29        73\n",
      "weighted avg       0.46      0.53      0.49        73\n",
      "\n",
      "Computing kernel between full-year data and training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sugeethjsa/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/sugeethjsa/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/sugeethjsa/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RENEWABLE ENERGY RECOMMENDATION\n",
      "==================================================\n",
      "Primary recommendation: Wind (score 174.36)\n",
      "==================================================\n",
      "Saved daily_probabilities.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hybrid Quantum-Classical System for Renewable Energy Source Classification\n",
    "\n",
    "- Uses a Quantum Reservoir (QRC) for feature extraction and a quantum-inspired\n",
    "  kernel (via PennyLane feature-state circuits) used with a classical SVM.\n",
    "- By default uses generated mock weather data so the pipeline runs end-to-end.\n",
    "- If you want to plug in a Kaggle dataset later, replace `download_kaggle_dataset`\n",
    "  or provide a local CSV folder path to `load_and_preprocess_data`.\n",
    "\n",
    "Dependencies:\n",
    "    pip install numpy matplotlib scikit-learn pandas pennylane\n",
    "\n",
    "Author: Cleaned and corrected version (beginner-friendly).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "# ---------------------------\n",
    "# Config / Global variables\n",
    "# ---------------------------\n",
    "HAS_RIVER_IN_AREA = True\n",
    "\n",
    "N_QUBITS = 5\n",
    "N_QUBITS_QSVC = 4\n",
    "\n",
    "# Quantum devices\n",
    "dev_qrc = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
    "dev_qsvc = qml.device(\"default.qubit\", wires=N_QUBITS_QSVC)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Data loading / preprocessing\n",
    "# ---------------------------\n",
    "\n",
    "def download_kaggle_dataset():\n",
    "    \"\"\"\n",
    "    Placeholder for Kaggle dataset download.\n",
    "\n",
    "    - If you have the Kaggle CLI/API set up, implement this to download and\n",
    "      return a local path. For now the function raises NotImplementedError\n",
    "      so the main() will fallback to mock data.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Kaggle download not implemented in this environment. \"\n",
    "        \"Either put CSV files in a folder and pass the path to \"\n",
    "        \"load_and_preprocess_data, or edit this function to use the Kaggle API.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(dataset_path=None, max_days=365):\n",
    "    \"\"\"\n",
    "    Loads CSV files from dataset_path (if provided) and preprocesses.\n",
    "    Accepts either:\n",
    "      - a full path to a single CSV file, or\n",
    "      - a directory containing CSV files.\n",
    "\n",
    "    If no dataset_path or no CSVs found, it falls back to synthetic mock data.\n",
    "\n",
    "    Returns:\n",
    "        features: (n_samples, 3)\n",
    "        labels: (n_samples,)\n",
    "    \"\"\"\n",
    "    if dataset_path:\n",
    "        csv_files = []\n",
    "\n",
    "        # If the user passed a single CSV file path, handle it directly\n",
    "        if os.path.isfile(dataset_path) and dataset_path.lower().endswith(\".csv\"):\n",
    "            csv_files = [dataset_path]\n",
    "        else:\n",
    "            # Otherwise walk the directory tree and collect CSVs\n",
    "            for root, dirs, files in os.walk(dataset_path):\n",
    "                for f in files:\n",
    "                    if f.lower().endswith(\".csv\"):\n",
    "                        csv_files.append(os.path.join(root, f))\n",
    "\n",
    "        if csv_files:\n",
    "            # Load the first CSV and attempt to map columns\n",
    "            df = pd.read_csv(csv_files[0])\n",
    "            print(f\"Loaded {csv_files[0]} with shape {df.shape}\")\n",
    "            return _preprocess_dataframe(df, max_days)\n",
    "\n",
    "    # fallback\n",
    "    print(\"No dataset available or dataset path empty — generating mock data.\")\n",
    "    return generate_mock_data(days=max_days)\n",
    "\n",
    "\n",
    "\n",
    "def _preprocess_dataframe(df, max_days=365):\n",
    "    # identify possible columns for features\n",
    "    n_samples = len(df)\n",
    "    features_dict = {'solar_irradiance': None, 'wind_speed': None, 'water_flow': None}\n",
    "\n",
    "    # Solar irradiance: try many possible temperature-like columns\n",
    "    temp_candidates = ['Temperature_C', 'temperature', 'Temperature', 'temp_C', 'temp', 'Temperature (C)']\n",
    "    for c in temp_candidates:\n",
    "        if c in df.columns:\n",
    "            features_dict['solar_irradiance'] = (df[c].astype(float) + 30) / 80.0\n",
    "            break\n",
    "\n",
    "    # Wind speed candidates\n",
    "    wind_candidates = ['Wind_Speed_kmh', 'wind_speed', 'Wind Speed_km/h', 'wind_kmph', 'wind_kmh', 'Wind Speed (km/h)']\n",
    "    for c in wind_candidates:\n",
    "        if c in df.columns:\n",
    "            features_dict['wind_speed'] = df[c].astype(float) / 100.0\n",
    "            break\n",
    "\n",
    "    # Water flow approximated from humidity or precipitation\n",
    "    if 'Humidity' in df.columns:\n",
    "        features_dict['water_flow'] = df['Humidity'].astype(float) / 100.0\n",
    "    elif 'humidity' in df.columns:\n",
    "        features_dict['water_flow'] = df['humidity'].astype(float) / 100.0\n",
    "    elif 'Precipitation' in df.columns:\n",
    "        # normalize precipitation heuristically\n",
    "        features_dict['water_flow'] = np.tanh(df['Precipitation'].astype(float) / 10.0)\n",
    "\n",
    "    # fill missing feature columns with random values\n",
    "    for k in features_dict:\n",
    "        if features_dict[k] is None:\n",
    "            print(f\"Warning: could not find column for {k}, generating synthetic values\")\n",
    "            features_dict[k] = np.random.rand(n_samples)\n",
    "        else:\n",
    "            # ensure it's an ndarray\n",
    "            features_dict[k] = np.asarray(features_dict[k])\n",
    "\n",
    "    # combine into features matrix and clip to [0,1]\n",
    "    features = np.column_stack([\n",
    "        features_dict['solar_irradiance'],\n",
    "        features_dict['wind_speed'],\n",
    "        features_dict['water_flow']\n",
    "    ])\n",
    "    features = np.clip(features, 0.0, 1.0)\n",
    "\n",
    "    # trim or pad to max_days\n",
    "    if len(features) > max_days:\n",
    "        features = features[:max_days]\n",
    "\n",
    "    labels = np.array([get_label(f) for f in features])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def generate_mock_data(days=365):\n",
    "    \"\"\"\n",
    "    Generate synthetic seasonal-looking data for testing.\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 2 * np.pi, days)\n",
    "\n",
    "    solar_irradiance = 0.5 + 0.3 * np.sin(t) + 0.2 * np.random.rand(days)\n",
    "    wind_speed = 0.5 - 0.2 * np.sin(t) + 0.3 * np.random.rand(days)\n",
    "    water_flow = 0.4 + 0.25 * np.sin(t + np.pi / 4) + 0.35 * np.random.rand(days)\n",
    "\n",
    "    solar_irradiance = np.clip(solar_irradiance, 0, 1)\n",
    "    wind_speed = np.clip(wind_speed, 0, 1)\n",
    "    water_flow = np.clip(water_flow, 0, 1)\n",
    "\n",
    "    features = np.column_stack([solar_irradiance, wind_speed, water_flow])\n",
    "    labels = np.array([get_label(f) for f in features])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def get_label(features):\n",
    "    \"\"\"\n",
    "    Map feature triple to a label string.\n",
    "    Uses the global HAS_RIVER_IN_AREA to consider Hydro feasibility.\n",
    "    \"\"\"\n",
    "    solar_irr, wind_spd, water_flw = features\n",
    "    DOMINANCE_THRESHOLD = 0.4\n",
    "    max_feature = max(solar_irr, wind_spd, water_flw)\n",
    "\n",
    "    if solar_irr == max_feature and solar_irr > DOMINANCE_THRESHOLD:\n",
    "        return \"Solar\"\n",
    "    elif wind_spd == max_feature and wind_spd > DOMINANCE_THRESHOLD:\n",
    "        return \"Wind\"\n",
    "    elif water_flw == max_feature and water_flw > DOMINANCE_THRESHOLD:\n",
    "        if HAS_RIVER_IN_AREA:\n",
    "            return \"Hydro\"\n",
    "        else:\n",
    "            return \"Solar\" if solar_irr > wind_spd else \"Wind\"\n",
    "    elif solar_irr < 0.3 and wind_spd < 0.3 and water_flw < 0.3:\n",
    "        return \"Geothermal\"\n",
    "    else:\n",
    "        return \"Mixed\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Quantum Reservoir Computer\n",
    "# ---------------------------\n",
    "\n",
    "def encode_input(features, n_qubits):\n",
    "    \"\"\"\n",
    "    Expand 3 features to n_qubits rotation angles (0..2pi)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros(n_qubits)\n",
    "    for i in range(n_qubits):\n",
    "        encoded[i] = features[i % len(features)]\n",
    "    return encoded * 2 * np.pi\n",
    "\n",
    "\n",
    "@qml.qnode(dev_qrc)\n",
    "def qrc_circuit(params, weights):\n",
    "    \"\"\"\n",
    "    QRC circuit: encode with RY and apply StronglyEntanglingLayers.\n",
    "    Returns expectation values <Z> per qubit.\n",
    "    \"\"\"\n",
    "    # input rotations\n",
    "    for i in range(N_QUBITS):\n",
    "        qml.RY(float(params[i]), wires=i)\n",
    "\n",
    "    # Strongly entangling layers expect shape (n_layers, n_wires, 3)\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(N_QUBITS))\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]\n",
    "\n",
    "\n",
    "def run_qrc(daily_features_list, n_layers=2):\n",
    "    \"\"\"\n",
    "    Run QRC across the sequence to get reservoir states.\n",
    "    \"\"\"\n",
    "    n_days = len(daily_features_list)\n",
    "    # small random init for StronglyEntanglingWeights\n",
    "    # shape: (n_layers, n_wires, 3)\n",
    "    weights = 0.1 * np.random.randn(n_layers, N_QUBITS, 3)\n",
    "\n",
    "    reservoir_state = np.zeros(N_QUBITS)\n",
    "    reservoir_states = []\n",
    "\n",
    "    print(f\"Running QRC on {n_days} days... (this may be slow with many qubits/layers)\")\n",
    "    for idx, features in enumerate(daily_features_list):\n",
    "        encoded = encode_input(features, N_QUBITS)\n",
    "        combined = encoded + 0.5 * reservoir_state\n",
    "        new_state = qrc_circuit(combined, weights)\n",
    "        reservoir_state = np.array(new_state, dtype=float)\n",
    "        reservoir_states.append(reservoir_state.copy())\n",
    "\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{n_days} days\")\n",
    "\n",
    "    return np.array(reservoir_states)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# GASF conversion\n",
    "# ---------------------------\n",
    "\n",
    "def vector_to_image(vector):\n",
    "    \"\"\"\n",
    "    Gramian Angular Summation Field for a 1D vector of length N -> NxN image\n",
    "    \"\"\"\n",
    "    v = np.array(vector, dtype=float)\n",
    "    minv, maxv = v.min(), v.max()\n",
    "    if np.isclose(maxv - minv, 0.0):\n",
    "        v_scaled = np.zeros_like(v)\n",
    "    else:\n",
    "        v_scaled = 2 * (v - minv) / (maxv - minv) - 1.0\n",
    "    v_scaled = np.clip(v_scaled, -1.0, 1.0)\n",
    "    angles = np.arccos(v_scaled)\n",
    "    n = len(angles)\n",
    "    gasf = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            gasf[i, j] = np.cos(angles[i] + angles[j])\n",
    "    return gasf\n",
    "\n",
    "\n",
    "def visualize_gasf_image(gasf_image, title=\"GASF_Image\"):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(gasf_image, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    fname = f\"gasf_{title.replace(' ', '_')}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved example GASF image as {fname}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Quantum feature map & kernel (QSVC)\n",
    "# ---------------------------\n",
    "\n",
    "@qml.qnode(dev_qsvc)\n",
    "def feature_state_circuit(features):\n",
    "    \"\"\"\n",
    "    Embed classical features (length <= N_QUBITS_QSVC) into a quantum state and return statevector.\n",
    "    \"\"\"\n",
    "    # ensure length exactly N_QUBITS_QSVC\n",
    "    features = np.asarray(features, dtype=float)\n",
    "    if len(features) < N_QUBITS_QSVC:\n",
    "        padded = np.zeros(N_QUBITS_QSVC)\n",
    "        padded[:len(features)] = features\n",
    "        features = padded\n",
    "    else:\n",
    "        features = features[:N_QUBITS_QSVC]\n",
    "\n",
    "    qml.AngleEmbedding(features, wires=range(N_QUBITS_QSVC))\n",
    "\n",
    "    # simple entangling chain\n",
    "    for i in range(N_QUBITS_QSVC - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "    return qml.state()\n",
    "\n",
    "\n",
    "def compute_kernel_matrix(X1, X2):\n",
    "    \"\"\"\n",
    "    Compute kernel matrix K where K[i,j] = |<psi(x_i) | psi(x_j)>|^2\n",
    "    using feature_state_circuit to produce statevectors.\n",
    "\n",
    "    This is expensive but okay for small datasets/demo.\n",
    "    \"\"\"\n",
    "    n1 = len(X1)\n",
    "    n2 = len(X2)\n",
    "    K = np.zeros((n1, n2), dtype=float)\n",
    "\n",
    "    # Precompute statevectors\n",
    "    states1 = [np.asarray(feature_state_circuit(x)) for x in X1]\n",
    "    states2 = [np.asarray(feature_state_circuit(x)) for x in X2]\n",
    "\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            overlap = np.vdot(states1[i], states2[j])\n",
    "            K[i, j] = np.abs(overlap) ** 2\n",
    "    return K\n",
    "\n",
    "\n",
    "def prepare_qsvc_data(reservoir_states, labels):\n",
    "    \"\"\"\n",
    "    Convert reservoir states -> GASF -> flatten -> PCA reduce to N_QUBITS_QSVC dims\n",
    "    and encode labels.\n",
    "    \"\"\"\n",
    "    gasf_images = [vector_to_image(s) for s in reservoir_states]\n",
    "    if len(gasf_images) > 0:\n",
    "        visualize_gasf_image(gasf_images[0], title=\"example_day1\")\n",
    "\n",
    "    X_flat = np.array([img.flatten() for img in gasf_images])\n",
    "    n_features_target = min(N_QUBITS_QSVC, X_flat.shape[1])\n",
    "    pca = PCA(n_components=n_features_target)\n",
    "    X_reduced = pca.fit_transform(X_flat)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(labels)\n",
    "    print(f\"Label mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "    return X_reduced, y_encoded, label_encoder, pca\n",
    "\n",
    "\n",
    "def train_qsvc(X_train, X_test, y_train, y_test, max_train_samples=100):\n",
    "    \"\"\"\n",
    "    Compute quantum kernel matrices and train classical SVM with precomputed kernel.\n",
    "    \"\"\"\n",
    "    print(f\"Training QSVC: {len(X_train)} train samples, {len(X_test)} test samples\")\n",
    "\n",
    "    # optionally limit training size for speed\n",
    "    if len(X_train) > max_train_samples:\n",
    "        idx = np.random.choice(len(X_train), max_train_samples, replace=False)\n",
    "        X_train_used = X_train[idx]\n",
    "        y_train_used = y_train[idx]\n",
    "    else:\n",
    "        X_train_used = X_train\n",
    "        y_train_used = y_train\n",
    "\n",
    "    print(\"Computing training kernel matrix (may take time)...\")\n",
    "    K_train = compute_kernel_matrix(X_train_used, X_train_used)\n",
    "\n",
    "    svc = SVC(kernel=\"precomputed\", probability=True, random_state=42)\n",
    "    svc.fit(K_train, y_train_used)\n",
    "\n",
    "    print(\"Computing test kernel matrix...\")\n",
    "    K_test = compute_kernel_matrix(X_test, X_train_used)\n",
    "\n",
    "    predictions = svc.predict(K_test)\n",
    "    probabilities = svc.predict_proba(K_test)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    return svc, K_train, predictions, probabilities, X_train_used\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Analysis & visualization\n",
    "# ---------------------------\n",
    "\n",
    "def analyze_full_year(features, reservoir_states, svc, X_train_used, label_encoder, pca):\n",
    "    \"\"\"\n",
    "    Compute predictions for all days and summarize annual suitability.\n",
    "    \"\"\"\n",
    "    n_days = len(features)\n",
    "    gasf_images = [vector_to_image(s) for s in reservoir_states]\n",
    "    X_full_flat = np.array([img.flatten() for img in gasf_images])\n",
    "    X_full_reduced = pca.transform(X_full_flat)\n",
    "\n",
    "    print(\"Computing kernel between full-year data and training set...\")\n",
    "    K_full = compute_kernel_matrix(X_full_reduced, X_train_used)\n",
    "\n",
    "    daily_probs = svc.predict_proba(K_full)\n",
    "    daily_pred = svc.predict(K_full)\n",
    "    classes = label_encoder.classes_\n",
    "\n",
    "    total_scores = {c: daily_probs[:, i].sum() for i, c in enumerate(classes)}\n",
    "\n",
    "    # find best 30-day windows\n",
    "    window_size = min(30, n_days)\n",
    "    best_windows = {}\n",
    "    for i, c in enumerate(classes):\n",
    "        probs = daily_probs[:, i]\n",
    "        best_avg = -1\n",
    "        best_start = 0\n",
    "        for start in range(0, n_days - window_size + 1):\n",
    "            avg = probs[start:start + window_size].mean()\n",
    "            if avg > best_avg:\n",
    "                best_avg = avg\n",
    "                best_start = start\n",
    "        best_windows[c] = {'start_day': int(best_start), 'avg_score': float(best_avg)}\n",
    "\n",
    "    sorted_types = sorted(total_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    primary_type, primary_score = sorted_types[0]\n",
    "\n",
    "    recommendation = {\n",
    "        'primary_type': primary_type,\n",
    "        'primary_score': float(primary_score),\n",
    "        'all_scores': total_scores,\n",
    "        'best_windows': best_windows,\n",
    "        'daily_probabilities': daily_probs,\n",
    "        'daily_predictions': daily_pred,\n",
    "        'classes': classes\n",
    "    }\n",
    "\n",
    "    # secondary if within 90%\n",
    "    if len(sorted_types) > 1:\n",
    "        sec_type, sec_score = sorted_types[1]\n",
    "        if sec_score >= 0.9 * primary_score:\n",
    "            recommendation['secondary_type'] = sec_type\n",
    "            recommendation['secondary_score'] = float(sec_score)\n",
    "\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "def print_final_recommendation(recommendation):\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"RENEWABLE ENERGY RECOMMENDATION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Primary recommendation: {recommendation['primary_type']} (score {recommendation['primary_score']:.2f})\")\n",
    "    if 'secondary_type' in recommendation:\n",
    "        print(f\"Secondary: {recommendation['secondary_type']} (score {recommendation['secondary_score']:.2f})\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "def visualize_annual_trends(recommendation):\n",
    "    daily_probs = recommendation['daily_probabilities']\n",
    "    classes = recommendation['classes']\n",
    "    n_days = daily_probs.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    days = np.arange(n_days)\n",
    "    for i, c in enumerate(classes):\n",
    "        plt.plot(days, daily_probs[:, i], label=c)\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Daily probabilities\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"daily_probabilities.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved daily_probabilities.png\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "\n",
    "def main(use_kaggle=False, kaggle_path=None):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    print(\"HYBRID QUANTUM-CLASSICAL RENEWABLE ENERGY CLASSIFIER (demo)\")\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        if use_kaggle:\n",
    "            print(\"📦 Using Kaggle dataset...\")\n",
    "            dataset_path = download_kaggle_dataset()\n",
    "            features, labels = load_and_preprocess_data(dataset_path)\n",
    "        elif kaggle_path and os.path.exists(kaggle_path):\n",
    "            print(f\"✅ Using local dataset: {kaggle_path}\")\n",
    "            features, labels = load_and_preprocess_data(kaggle_path)\n",
    "        else:\n",
    "            print(\"⚠️ No dataset available or invalid path — generating mock data.\")\n",
    "            features, labels = generate_mock_data(days=365)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Dataset load failed — generating mock data instead. Reason: {e}\")\n",
    "        features, labels = generate_mock_data(days=365)\n",
    "\n",
    "    print(f\"Loaded {len(features)} days; unique labels: {np.unique(labels)}\")\n",
    "\n",
    "    # QRC\n",
    "    reservoir_states = run_qrc(features, n_layers=2)\n",
    "    print(f\"Reservoir states shape: {reservoir_states.shape}\")\n",
    "\n",
    "    # Prepare QSVC data\n",
    "    X_reduced, y_encoded, label_encoder, pca = prepare_qsvc_data(reservoir_states, labels)\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reduced, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    svc, K_train, preds, probs, X_train_used = train_qsvc(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    recommendation = analyze_full_year(features, reservoir_states, svc, X_train_used, label_encoder, pca)\n",
    "\n",
    "    print_final_recommendation(recommendation)\n",
    "    visualize_annual_trends(recommendation)\n",
    "\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = os.path.join(os.getcwd(), \"weatherHistory.csv\")\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"❌ File not found:\", csv_path)\n",
    "    else:\n",
    "        print(\"✅ Using dataset:\", csv_path)\n",
    "\n",
    "    recommendation = main(use_kaggle=False, kaggle_path=csv_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
