{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14503a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hybrid Quantum-Classical System for Renewable Energy Source Classification\n",
    "\n",
    "This system uses Quantum Reservoir Computing (QRC) for feature extraction and\n",
    "Quantum Support Vector Classifier (QSVC) for classification of renewable energy sources.\n",
    "\n",
    "Technologies:\n",
    "- PennyLane: For quantum circuits (QRC and QSVC)\n",
    "- Scikit-learn: For classical SVC and metrics\n",
    "- NumPy: For numerical operations\n",
    "- Matplotlib: For visualization\n",
    "- Kaggle Dataset: Real weather data\n",
    "\n",
    "Author: Beginner-friendly implementation with detailed comments\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 1: SETUP AND DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "# Global constraint: Does the area have access to water resources?\n",
    "HAS_RIVER_IN_AREA = True\n",
    "\n",
    "def download_kaggle_dataset():\n",
    "    \"\"\"\n",
    "    Downloads the weather dataset from Kaggle.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded dataset\n",
    "    \"\"\"\n",
    "    print(\"Downloading Kaggle weather dataset...\")\n",
    "    path = kagglehub.dataset_download(\"muthuj7/weather-dataset\")\n",
    "    print(f\"Dataset downloaded to: {path}\")\n",
    "    return path\n",
    "\n",
    "def load_and_preprocess_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the weather dataset.\n",
    "\n",
    "    The function reads CSV files from the dataset, extracts relevant weather features,\n",
    "    and prepares them for quantum machine learning processing.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset directory\n",
    "\n",
    "    Returns:\n",
    "        tuple: (features, labels) where features is shape (n_samples, 3) and labels is shape (n_samples,)\n",
    "    \"\"\"\n",
    "    print(\"Loading and preprocessing weather data...\")\n",
    "\n",
    "    # Find CSV files in the dataset directory\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found. Generating mock data instead.\")\n",
    "        return generate_mock_data(days=365)\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV file(s)\")\n",
    "\n",
    "    # Load the first CSV file (you can modify this to combine multiple files)\n",
    "    df = pd.read_csv(csv_files[0])\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Extract relevant features based on available columns\n",
    "    # Common weather dataset columns: temperature, humidity, wind_speed, pressure, etc.\n",
    "\n",
    "    features_dict = {\n",
    "        'solar_irradiance': None,\n",
    "        'wind_speed': None,\n",
    "        'water_flow': None\n",
    "    }\n",
    "\n",
    "    # Map dataset columns to our required features\n",
    "    # Solar irradiance approximation: use temperature, humidity, and cloud cover\n",
    "    if 'Temperature_C' in df.columns or 'temperature' in df.columns:\n",
    "        temp_col = 'Temperature_C' if 'Temperature_C' in df.columns else 'temperature'\n",
    "        # Normalize temperature to 0-1 range (assuming -30 to 50 Celsius range)\n",
    "        features_dict['solar_irradiance'] = (df[temp_col] + 30) / 80\n",
    "    elif 'Temperature' in df.columns:\n",
    "        features_dict['solar_irradiance'] = (df['Temperature'] + 30) / 80\n",
    "\n",
    "    # Wind speed\n",
    "    if 'Wind_Speed_kmh' in df.columns:\n",
    "        # Normalize wind speed (0-100 km/h range)\n",
    "        features_dict['wind_speed'] = df['Wind_Speed_kmh'] / 100\n",
    "    elif 'wind_speed' in df.columns:\n",
    "        features_dict['wind_speed'] = df['wind_speed'] / 100\n",
    "    elif 'Wind Speed_km/h' in df.columns:\n",
    "        features_dict['wind_speed'] = df['Wind Speed_km/h'] / 100\n",
    "\n",
    "    # Water flow approximation: use humidity and precipitation\n",
    "    if 'Humidity' in df.columns:\n",
    "        # Normalize humidity (0-100% range)\n",
    "        features_dict['water_flow'] = df['Humidity'] / 100\n",
    "    elif 'humidity' in df.columns:\n",
    "        features_dict['water_flow'] = df['humidity'] / 100\n",
    "\n",
    "    # If any features are missing, generate synthetic data\n",
    "    n_samples = len(df)\n",
    "    for key, value in features_dict.items():\n",
    "        if value is None:\n",
    "            print(f\"Warning: {key} not found in dataset, generating synthetic data\")\n",
    "            features_dict[key] = np.random.rand(n_samples)\n",
    "\n",
    "    # Combine features into a single array\n",
    "    features = np.column_stack([\n",
    "        features_dict['solar_irradiance'],\n",
    "        features_dict['wind_speed'],\n",
    "        features_dict['water_flow']\n",
    "    ])\n",
    "\n",
    "    # Clip values to 0-1 range\n",
    "    features = np.clip(features, 0, 1)\n",
    "\n",
    "    # Limit to 365 days if more data is available\n",
    "    if len(features) > 365:\n",
    "        features = features[:365]\n",
    "\n",
    "    # Generate labels based on the features\n",
    "    labels = np.array([get_label(feat) for feat in features])\n",
    "\n",
    "    print(f\"Preprocessed data shape: {features.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(labels)}\")\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def generate_mock_data(days=365):\n",
    "    \"\"\"\n",
    "    Generates synthetic weather data for testing when real data is unavailable.\n",
    "\n",
    "    This function creates realistic-looking weather patterns with seasonal variations\n",
    "    for solar irradiance, wind speed, and water flow.\n",
    "\n",
    "    Args:\n",
    "        days (int): Number of days to generate data for\n",
    "\n",
    "    Returns:\n",
    "        tuple: (features, labels) where features is shape (days, 3) and labels is shape (days,)\n",
    "    \"\"\"\n",
    "    print(f\"Generating mock data for {days} days...\")\n",
    "\n",
    "    # Create time array for seasonal patterns\n",
    "    t = np.linspace(0, 2 * np.pi, days)\n",
    "\n",
    "    # Solar irradiance: Higher in summer, lower in winter\n",
    "    # Base sine wave + random noise\n",
    "    solar_irradiance = 0.5 + 0.3 * np.sin(t) + 0.2 * np.random.rand(days)\n",
    "\n",
    "    # Wind speed: More variable, slight seasonal pattern\n",
    "    # Inverse sine (higher in winter) + more noise\n",
    "    wind_speed = 0.5 - 0.2 * np.sin(t) + 0.3 * np.random.rand(days)\n",
    "\n",
    "    # Water flow: Related to precipitation patterns\n",
    "    # Complex seasonal pattern + noise\n",
    "    water_flow = 0.4 + 0.25 * np.sin(t + np.pi/4) + 0.35 * np.random.rand(days)\n",
    "\n",
    "    # Clip all values to [0, 1] range\n",
    "    solar_irradiance = np.clip(solar_irradiance, 0, 1)\n",
    "    wind_speed = np.clip(wind_speed, 0, 1)\n",
    "    water_flow = np.clip(water_flow, 0, 1)\n",
    "\n",
    "    # Stack features into a single array\n",
    "    features = np.column_stack([solar_irradiance, wind_speed, water_flow])\n",
    "\n",
    "    # Generate labels based on the features\n",
    "    labels = np.array([get_label(feat) for feat in features])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def get_label(features):\n",
    "    \"\"\"\n",
    "    Determines the most suitable renewable energy source based on weather features.\n",
    "\n",
    "    Logic:\n",
    "    - Solar: Best when solar irradiance is dominant\n",
    "    - Wind: Best when wind speed is dominant\n",
    "    - Hydro: Best when water flow is dominant AND HAS_RIVER_IN_AREA is True\n",
    "    - Geothermal: Stable baseline option (can be added)\n",
    "    - Mixed: When no single source is clearly dominant\n",
    "\n",
    "    Args:\n",
    "        features (array): [solar_irradiance, wind_speed, water_flow]\n",
    "\n",
    "    Returns:\n",
    "        str: Energy source label\n",
    "    \"\"\"\n",
    "    solar_irr, wind_spd, water_flw = features\n",
    "\n",
    "    # Define threshold for \"dominant\" feature\n",
    "    DOMINANCE_THRESHOLD = 0.4\n",
    "\n",
    "    # Check for dominant features\n",
    "    max_feature = max(solar_irr, wind_spd, water_flw)\n",
    "\n",
    "    # Solar is dominant\n",
    "    if solar_irr == max_feature and solar_irr > DOMINANCE_THRESHOLD:\n",
    "        return \"Solar\"\n",
    "\n",
    "    # Wind is dominant\n",
    "    elif wind_spd == max_feature and wind_spd > DOMINANCE_THRESHOLD:\n",
    "        return \"Wind\"\n",
    "\n",
    "    # Water/Hydro is dominant\n",
    "    elif water_flw == max_feature and water_flw > DOMINANCE_THRESHOLD:\n",
    "        if HAS_RIVER_IN_AREA:\n",
    "            return \"Hydro\"\n",
    "        else:\n",
    "            # If no river, fall back to next best option\n",
    "            if solar_irr > wind_spd:\n",
    "                return \"Solar\"\n",
    "            else:\n",
    "                return \"Wind\"\n",
    "\n",
    "    # No clear dominant feature - check for secondary options\n",
    "    # Geothermal: Stable baseline (assume moderate conditions)\n",
    "    elif solar_irr < 0.3 and wind_spd < 0.3 and water_flw < 0.3:\n",
    "        return \"Geothermal\"\n",
    "\n",
    "    # Mixed: Multiple good options available\n",
    "    else:\n",
    "        return \"Mixed\"\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 2: QUANTUM RESERVOIR COMPUTER (QRC)\n",
    "# =============================================================================\n",
    "\n",
    "# Define number of qubits for the reservoir\n",
    "N_QUBITS = 5\n",
    "\n",
    "# Create quantum device\n",
    "# default.qubit is a simulator that runs on classical hardware\n",
    "dev_qrc = qml.device('default.qubit', wires=N_QUBITS)\n",
    "\n",
    "def encode_input(features, n_qubits):\n",
    "    \"\"\"\n",
    "    Encodes classical input features into quantum circuit parameters.\n",
    "\n",
    "    This function maps 3 weather features to n_qubits parameters by repeating\n",
    "    and scaling the features. This is a simple encoding scheme suitable for\n",
    "    beginners.\n",
    "\n",
    "    Args:\n",
    "        features (array): Weather features [solar, wind, water]\n",
    "        n_qubits (int): Number of qubits in the quantum circuit\n",
    "\n",
    "    Returns:\n",
    "        array: Encoded parameters of length n_qubits\n",
    "    \"\"\"\n",
    "    # Repeat features to match number of qubits\n",
    "    # For example, if n_qubits=5 and features has 3 elements,\n",
    "    # we cycle through features: [f0, f1, f2, f0, f1]\n",
    "    encoded = np.zeros(n_qubits)\n",
    "    for i in range(n_qubits):\n",
    "        encoded[i] = features[i % len(features)]\n",
    "\n",
    "    # Scale to quantum rotation angles (0 to 2π)\n",
    "    encoded = encoded * 2 * np.pi\n",
    "\n",
    "    return encoded\n",
    "\n",
    "@qml.qnode(dev_qrc)\n",
    "def qrc_circuit(params, weights):\n",
    "    \"\"\"\n",
    "    Quantum Reservoir Circuit using StronglyEntanglingLayers.\n",
    "\n",
    "    This circuit represents the \"quantum dynamics\" of the reservoir.\n",
    "    It takes the current reservoir state and input features, applies\n",
    "    quantum gates to create entanglement, and produces a new reservoir state.\n",
    "\n",
    "    Args:\n",
    "        params (array): Encoded input features (rotation angles)\n",
    "        weights (array): Trainable weights for entangling layers\n",
    "\n",
    "    Returns:\n",
    "        list: Expectation values of PauliZ for each qubit (the new reservoir state)\n",
    "    \"\"\"\n",
    "    # Step 1: Encode input features as rotation gates on each qubit\n",
    "    # RY rotation represents the input signal\n",
    "    for i in range(N_QUBITS):\n",
    "        qml.RY(params[i], wires=i)\n",
    "\n",
    "    # Step 2: Apply strongly entangling layers\n",
    "    # This creates quantum entanglement between qubits, allowing the reservoir\n",
    "    # to capture complex patterns in the data\n",
    "    # StronglyEntanglingLayers applies rotations and CNOTs to create entanglement\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(N_QUBITS))\n",
    "\n",
    "    # Step 3: Measure the expectation values\n",
    "    # PauliZ measurements give values between -1 and +1\n",
    "    # These become the new reservoir state\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]\n",
    "\n",
    "def run_qrc(daily_features_list, n_layers=2):\n",
    "    \"\"\"\n",
    "    Runs the Quantum Reservoir Computer on a sequence of daily weather data.\n",
    "\n",
    "    This is the main QRC driver function. It processes each day's weather features\n",
    "    sequentially, maintaining a reservoir state that captures temporal patterns.\n",
    "\n",
    "    Args:\n",
    "        daily_features_list (array): Array of shape (n_days, 3) with daily weather features\n",
    "        n_layers (int): Number of entangling layers in the quantum circuit\n",
    "\n",
    "    Returns:\n",
    "        array: Reservoir states for each day, shape (n_days, N_QUBITS)\n",
    "    \"\"\"\n",
    "    n_days = len(daily_features_list)\n",
    "\n",
    "    # Initialize random weights for the entangling layers\n",
    "    # Shape: (n_layers, n_qubits, 3) because StronglyEntanglingLayers uses 3 rotations per qubit\n",
    "    weights = pnp.random.randn(n_layers, N_QUBITS, 3, requires_grad=False) * 0.1\n",
    "\n",
    "    # Initialize reservoir state (start with zeros)\n",
    "    reservoir_state = np.zeros(N_QUBITS)\n",
    "\n",
    "    # Store all reservoir states\n",
    "    reservoir_states = []\n",
    "\n",
    "    print(f\"Running QRC on {n_days} days of data...\")\n",
    "\n",
    "    # Process each day sequentially\n",
    "    for day_idx, features in enumerate(daily_features_list):\n",
    "        # Encode the day's weather features\n",
    "        encoded_input = encode_input(features, N_QUBITS)\n",
    "\n",
    "        # Combine encoded input with previous reservoir state\n",
    "        # This allows the reservoir to maintain memory of past states\n",
    "        combined_params = encoded_input + reservoir_state * 0.5\n",
    "\n",
    "        # Run the quantum circuit to get new reservoir state\n",
    "        new_state = qrc_circuit(combined_params, weights)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        reservoir_state = np.array(new_state)\n",
    "\n",
    "        # Store the state\n",
    "        reservoir_states.append(reservoir_state.copy())\n",
    "\n",
    "        # Progress indicator\n",
    "        if (day_idx + 1) % 50 == 0:\n",
    "            print(f\"  Processed {day_idx + 1}/{n_days} days\")\n",
    "\n",
    "    return np.array(reservoir_states)\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 3: GRAMIAN ANGULAR FIELD TRANSFORMATION\n",
    "# =============================================================================\n",
    "\n",
    "def vector_to_image(vector):\n",
    "    \"\"\"\n",
    "    Converts a 1D reservoir state vector to a 2D image using Gramian Angular Summation Field.\n",
    "\n",
    "    The GASF transformation creates a 2D representation that captures temporal correlations\n",
    "    within the time series data. This \"image\" can then be classified by the QSVC.\n",
    "\n",
    "    Mathematical process:\n",
    "    1. Rescale vector values to [-1, 1] range\n",
    "    2. Calculate arccos to get angles (Polar Coordinate Transformation)\n",
    "    3. Compute GASF matrix: GASF[i,j] = cos(angle[i] + angle[j])\n",
    "\n",
    "    Args:\n",
    "        vector (array): 1D reservoir state vector\n",
    "\n",
    "    Returns:\n",
    "        array: 2D GASF matrix (image representation)\n",
    "    \"\"\"\n",
    "    # Step 1: Rescale to [-1, 1] range\n",
    "    # Min-max normalization then scale to [-1, 1]\n",
    "    min_val = np.min(vector)\n",
    "    max_val = np.max(vector)\n",
    "\n",
    "    if max_val - min_val < 1e-10:  # Avoid division by zero\n",
    "        vector_scaled = np.zeros_like(vector)\n",
    "    else:\n",
    "        vector_scaled = 2 * (vector - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "    # Clip to ensure values are in [-1, 1] (for arccos stability)\n",
    "    vector_scaled = np.clip(vector_scaled, -1, 1)\n",
    "\n",
    "    # Step 2: Convert to angles (Polar Coordinate Transformation)\n",
    "    # arccos maps [-1, 1] to [0, π]\n",
    "    angles = np.arccos(vector_scaled)\n",
    "\n",
    "    # Step 3: Compute GASF matrix\n",
    "    # GASF[i,j] = cos(angle[i] + angle[j])\n",
    "    # This captures the correlation between different parts of the time series\n",
    "    n = len(angles)\n",
    "    gasf = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            gasf[i, j] = np.cos(angles[i] + angles[j])\n",
    "\n",
    "    return gasf\n",
    "\n",
    "def visualize_gasf_image(gasf_image, title=\"GASF Image\"):\n",
    "    \"\"\"\n",
    "    Visualizes a GASF image using matplotlib.\n",
    "\n",
    "    Args:\n",
    "        gasf_image (array): 2D GASF matrix\n",
    "        title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(gasf_image, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='GASF Value')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Time Index')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'gasf_image_{title.replace(\" \", \"_\")}.png')\n",
    "    print(f\"Saved GASF visualization as gasf_image_{title.replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 4: QUANTUM SUPPORT VECTOR CLASSIFIER (QSVC)\n",
    "# =============================================================================\n",
    "\n",
    "# Define quantum device for QSVC\n",
    "N_QUBITS_QSVC = 4  # Using fewer qubits for the feature map to reduce computation\n",
    "dev_qsvc = qml.device('default.qubit', wires=N_QUBITS_QSVC)\n",
    "\n",
    "@qml.qnode(dev_qsvc)\n",
    "def quantum_feature_map(features):\n",
    "    \"\"\"\n",
    "    Quantum feature map circuit for the QSVC kernel.\n",
    "\n",
    "    This circuit embeds classical data (flattened GASF images) into a quantum state.\n",
    "    The quantum kernel then computes similarity between data points by measuring\n",
    "    the overlap of their quantum states.\n",
    "\n",
    "    We use AngleEmbedding to map features to rotation angles on qubits.\n",
    "\n",
    "    Args:\n",
    "        features (array): Flattened GASF image (must match or be truncated to N_QUBITS_QSVC)\n",
    "\n",
    "    Returns:\n",
    "        array: Quantum state (not used directly, but kernel computes overlap)\n",
    "    \"\"\"\n",
    "    # Truncate or pad features to match number of qubits\n",
    "    if len(features) > N_QUBITS_QSVC:\n",
    "        features = features[:N_QUBITS_QSVC]\n",
    "    elif len(features) < N_QUBITS_QSVC:\n",
    "        features = np.pad(features, (0, N_QUBITS_QSVC - len(features)), mode='constant')\n",
    "\n",
    "    # Angle embedding: each feature becomes a rotation angle\n",
    "    qml.AngleEmbedding(features, wires=range(N_QUBITS_QSVC))\n",
    "\n",
    "    # Add entangling layer to increase expressiveness\n",
    "    for i in range(N_QUBITS_QSVC - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "    # Return the statevector for kernel computation\n",
    "    return qml.state()\n",
    "\n",
    "def prepare_qsvc_data(reservoir_states, labels):\n",
    "    \"\"\"\n",
    "    Prepares data for QSVC by converting reservoir states to GASF images and flattening.\n",
    "\n",
    "    Args:\n",
    "        reservoir_states (array): Output from QRC, shape (n_samples, N_QUBITS)\n",
    "        labels (array): Energy source labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_flat, y_encoded) where X_flat is flattened images and y_encoded is integer labels\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for QSVC...\")\n",
    "\n",
    "    # Convert each reservoir state to a GASF image\n",
    "    gasf_images = []\n",
    "    for state in reservoir_states:\n",
    "        gasf_img = vector_to_image(state)\n",
    "        gasf_images.append(gasf_img)\n",
    "\n",
    "    # Visualize one example GASF image\n",
    "    if len(gasf_images) > 0:\n",
    "        visualize_gasf_image(gasf_images[0], title=\"Example GASF Image Day 1\")\n",
    "\n",
    "    # Flatten images for SVC input\n",
    "    # Each GASF image is (N_QUBITS x N_QUBITS), flatten to (N_QUBITS^2,)\n",
    "    X_flat = np.array([img.flatten() for img in gasf_images])\n",
    "\n",
    "    # Apply PCA or feature selection to reduce dimensionality for quantum circuit\n",
    "    # Since we only have N_QUBITS_QSVC qubits, we need to reduce features\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    n_features_target = min(N_QUBITS_QSVC, X_flat.shape[1])\n",
    "    pca = PCA(n_components=n_features_target)\n",
    "    X_reduced = pca.fit_transform(X_flat)\n",
    "\n",
    "    print(f\"Reduced feature dimensions from {X_flat.shape[1]} to {X_reduced.shape[1]}\")\n",
    "\n",
    "    # Encode labels as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "    print(f\"Label mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "    return X_reduced, y_encoded, label_encoder, pca\n",
    "\n",
    "def train_qsvc(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the Quantum Support Vector Classifier.\n",
    "\n",
    "    This function:\n",
    "    1. Computes quantum kernel matrices for training and test data\n",
    "    2. Trains a classical SVC using the quantum kernel\n",
    "    3. Evaluates the model on test data\n",
    "\n",
    "    Args:\n",
    "        X_train, X_test: Feature matrices\n",
    "        y_train, y_test: Label vectors\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_svc, kernel_train, predictions, probabilities)\n",
    "    \"\"\"\n",
    "    print(\"Training QSVC...\")\n",
    "    print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "    # Compute quantum kernel matrix for training data\n",
    "    print(\"Computing training kernel matrix...\")\n",
    "    # Note: This is computationally expensive for large datasets\n",
    "    # For demonstration, we'll use a subset if data is too large\n",
    "\n",
    "    max_train_samples = 100  # Limit for computational efficiency\n",
    "    if len(X_train) > max_train_samples:\n",
    "        print(f\"Limiting training data to {max_train_samples} samples for efficiency\")\n",
    "        indices = np.random.choice(len(X_train), max_train_samples, replace=False)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "    # Compute kernel matrix using quantum feature map\n",
    "    # This computes <phi(x_i)|phi(x_j)> for all pairs of training samples\n",
    "    kernel_train = qml.kernels.kernel_matrix(X_train, X_train, quantum_feature_map)\n",
    "\n",
    "    print(f\"Training kernel shape: {kernel_train.shape}\")\n",
    "\n",
    "    # Train classical SVC with precomputed quantum kernel\n",
    "    svc = SVC(kernel='precomputed', probability=True, random_state=42)\n",
    "    svc.fit(kernel_train, y_train)\n",
    "\n",
    "    print(\"Computing test kernel matrix...\")\n",
    "    # Compute kernel between test and training data\n",
    "    kernel_test = qml.kernels.kernel_matrix(X_test, X_train, quantum_feature_map)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = svc.predict(kernel_test)\n",
    "    probabilities = svc.predict_proba(kernel_test)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    return svc, kernel_train, predictions, probabilities, X_train\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 5: ANNUAL ANALYSIS AND RECOMMENDATION\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_full_year(features, reservoir_states, svc, X_train, label_encoder, pca):\n",
    "    \"\"\"\n",
    "    Analyzes a full year of data and generates energy source recommendations.\n",
    "\n",
    "    This function:\n",
    "    1. Converts all reservoir states to GASF images\n",
    "    2. Computes quantum kernel against training data\n",
    "    3. Gets daily probability scores for each energy source\n",
    "    4. Analyzes trends and identifies peak seasons\n",
    "    5. Generates final recommendations\n",
    "\n",
    "    Args:\n",
    "        features: Original weather features\n",
    "        reservoir_states: QRC output for all days\n",
    "        svc: Trained QSVC model\n",
    "        X_train: Training data (for kernel computation)\n",
    "        label_encoder: Label encoder for class names\n",
    "        pca: PCA transformer for dimensionality reduction\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results with recommendations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FULL YEAR ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    n_days = len(features)\n",
    "\n",
    "    # Convert all reservoir states to GASF images and flatten\n",
    "    print(\"Converting reservoir states to GASF images...\")\n",
    "    gasf_images = [vector_to_image(state) for state in reservoir_states]\n",
    "    X_full_flat = np.array([img.flatten() for img in gasf_images])\n",
    "\n",
    "    # Apply same PCA transformation as training data\n",
    "    X_full_reduced = pca.transform(X_full_flat)\n",
    "\n",
    "    # Compute kernel between full year data and training data\n",
    "    print(\"Computing full year kernel matrix...\")\n",
    "    kernel_full = qml.kernels.kernel_matrix(X_full_reduced, X_train, quantum_feature_map)\n",
    "\n",
    "    # Get daily probabilities for each energy source\n",
    "    daily_probabilities = svc.predict_proba(kernel_full)\n",
    "    daily_predictions = svc.predict(kernel_full)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = label_encoder.classes_\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    print(f\"\\nAnalyzing {n_days} days across {n_classes} energy sources...\")\n",
    "\n",
    "    # Calculate total scores for each energy type\n",
    "    total_scores = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        total_scores[class_name] = np.sum(daily_probabilities[:, i])\n",
    "\n",
    "    print(\"\\nTotal Annual Scores:\")\n",
    "    for energy_type, score in sorted(total_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {energy_type}: {score:.2f}\")\n",
    "\n",
    "    # Find best timeframes (30-day windows) for each energy type\n",
    "    window_size = 30\n",
    "    best_windows = {}\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Calculate rolling average for this energy type\n",
    "        class_probs = daily_probabilities[:, i]\n",
    "        best_avg = 0\n",
    "        best_start = 0\n",
    "\n",
    "        for start_day in range(n_days - window_size + 1):\n",
    "            window_avg = np.mean(class_probs[start_day:start_day + window_size])\n",
    "            if window_avg > best_avg:\n",
    "                best_avg = window_avg\n",
    "                best_start = start_day\n",
    "\n",
    "        # Convert day to month approximation\n",
    "        best_month = (best_start // 30) + 1\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        month_name = month_names[min(best_month - 1, 11)]\n",
    "\n",
    "        best_windows[class_name] = {\n",
    "            'start_day': best_start,\n",
    "            'month': month_name,\n",
    "            'avg_score': best_avg\n",
    "        }\n",
    "\n",
    "    print(\"\\nPeak Seasons:\")\n",
    "    for energy_type, window_info in best_windows.items():\n",
    "        print(f\"  {energy_type}: {window_info['month']} (avg score: {window_info['avg_score']:.3f})\")\n",
    "\n",
    "    # Generate recommendations\n",
    "    sorted_types = sorted(total_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    primary_type, primary_score = sorted_types[0]\n",
    "    primary_month = best_windows[primary_type]['month']\n",
    "\n",
    "    recommendation = {\n",
    "        'primary_type': primary_type,\n",
    "        'primary_score': primary_score,\n",
    "        'primary_peak_month': primary_month,\n",
    "        'secondary_type': None,\n",
    "        'secondary_score': None,\n",
    "        'secondary_peak_month': None,\n",
    "        'all_scores': total_scores,\n",
    "        'best_windows': best_windows,\n",
    "        'daily_probabilities': daily_probabilities,\n",
    "        'daily_predictions': daily_predictions\n",
    "    }\n",
    "\n",
    "    # Check for secondary recommendation (within 90% of top score)\n",
    "    if len(sorted_types) > 1:\n",
    "        secondary_type, secondary_score = sorted_types[1]\n",
    "        if secondary_score >= 0.9 * primary_score:\n",
    "            secondary_month = best_windows[secondary_type]['month']\n",
    "            recommendation['secondary_type'] = secondary_type\n",
    "            recommendation['secondary_score'] = secondary_score\n",
    "            recommendation['secondary_peak_month'] = secondary_month\n",
    "\n",
    "    return recommendation\n",
    "\n",
    "def print_final_recommendation(recommendation):\n",
    "    \"\"\"\n",
    "    Prints the final recommendation in a user-friendly format.\n",
    "\n",
    "    Args:\n",
    "        recommendation (dict): Analysis results from analyze_full_year\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RENEWABLE ENERGY RECOMMENDATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n🌟 PRIMARY RECOMMENDATION: {recommendation['primary_type']}\")\n",
    "    print(f\"   Total Annual Score: {recommendation['primary_score']:.2f}\")\n",
    "    print(f\"   Peak Season: {recommendation['primary_peak_month']}\")\n",
    "\n",
    "    if recommendation['secondary_type']:\n",
    "        print(f\"\\n⚡ SECONDARY RECOMMENDATION: {recommendation['secondary_type']}\")\n",
    "        print(f\"   Total Annual Score: {recommendation['secondary_score']:.2f}\")\n",
    "        print(f\"   Peak Season: {recommendation['secondary_peak_month']}\")\n",
    "        print(f\"\\n   Note: Secondary option is within 90% of primary score,\")\n",
    "        print(f\"         indicating strong viability as a complementary source.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "def visualize_annual_trends(recommendation, label_encoder):\n",
    "    \"\"\"\n",
    "    Creates visualizations of annual energy trends.\n",
    "\n",
    "    Args:\n",
    "        recommendation (dict): Analysis results\n",
    "        label_encoder: Label encoder for class names\n",
    "    \"\"\"\n",
    "    daily_probs = recommendation['daily_probabilities']\n",
    "    n_days = len(daily_probs)\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Plot 1: Daily probability trends\n",
    "    days = np.arange(n_days)\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        ax1.plot(days, daily_probs[:, i], label=class_name, linewidth=2)\n",
    "\n",
    "    ax1.set_xlabel('Day of Year')\n",
    "    ax1.set_ylabel('Probability Score')\n",
    "    ax1.set_title('Daily Renewable Energy Source Probabilities')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Total scores bar chart\n",
    "    scores = [recommendation['all_scores'][name] for name in class_names]\n",
    "    colors = ['#FDB462', '#80B1D3', '#B3DE69', '#FCCDE5', '#8DD3C7']\n",
    "    ax2.bar(class_names, scores, color=colors[:len(class_names)])\n",
    "    ax2.set_ylabel('Total Annual Score')\n",
    "    ax2.set_title('Total Annual Suitability by Energy Source')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('annual_energy_analysis.png', dpi=150)\n",
    "    print(\"\\nSaved visualization as annual_energy_analysis.png\")\n",
    "    plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function that orchestrates the entire quantum ML pipeline.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"HYBRID QUANTUM-CLASSICAL RENEWABLE ENERGY CLASSIFIER\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nThis system uses:\")\n",
    "    print(\"  • Quantum Reservoir Computing (QRC) for feature extraction\")\n",
    "    print(\"  • Gramian Angular Field (GAF) transformation\")\n",
    "    print(\"  • Quantum Support Vector Classifier (QSVC) for classification\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # PHASE 1: Load data\n",
    "    print(\"\\n[PHASE 1] Loading data...\")\n",
    "    try:\n",
    "        dataset_path = download_kaggle_dataset()\n",
    "        features, labels = load_and_preprocess_data(dataset_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Kaggle dataset: {e}\")\n",
    "        print(\"Falling back to mock data generation...\")\n",
    "        features, labels = generate_mock_data(days=365)\n",
    "\n",
    "    print(f\"Loaded {len(features)} days of weather data\")\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(labels)}\")\n",
    "\n",
    "    # PHASE 2: Run Quantum Reservoir Computer\n",
    "    print(\"\\n[PHASE 2] Running Quantum Reservoir Computer...\")\n",
    "    reservoir_states = run_qrc(features, n_layers=2)\n",
    "    print(f\"Generated reservoir states shape: {reservoir_states.shape}\")\n",
    "\n",
    "    # PHASE 3: Convert to GASF images\n",
    "    print(\"\\n[PHASE 3] Converting to GASF images...\")\n",
    "    X_processed, y_encoded, label_encoder, pca = prepare_qsvc_data(reservoir_states, labels)\n",
    "\n",
    "    # Apply constraint: Remove Hydro class if no river\n",
    "    if not HAS_RIVER_IN_AREA:\n",
    "        print(\"\\nApplying constraint: HAS_RIVER_IN_AREA = False\")\n",
    "        print(\"Filtering out 'Hydro' samples...\")\n",
    "        hydro_label = None\n",
    "        if 'Hydro' in label_encoder.classes_:\n",
    "            hydro_label = label_encoder.transform(['Hydro'])[0]\n",
    "            mask = y_encoded != hydro_label\n",
    "            X_processed = X_processed[mask]\n",
    "            y_encoded = y_encoded[mask]\n",
    "            print(f\"Removed {np.sum(~mask)} Hydro samples\")\n",
    "            print(f\"Remaining samples: {len(X_processed)}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    # PHASE 4: Train QSVC\n",
    "    print(\"\\n[PHASE 4] Training Quantum Support Vector Classifier...\")\n",
    "    svc, kernel_train, predictions, probabilities, X_train_used = train_qsvc(\n",
    "        X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    # PHASE 5: Full year analysis\n",
    "    print(\"\\n[PHASE 5] Analyzing full year and generating recommendations...\")\n",
    "    recommendation = analyze_full_year(\n",
    "        features, reservoir_states, svc, X_train_used, label_encoder, pca\n",
    "    )\n",
    "\n",
    "    # Print final recommendation\n",
    "    print_final_recommendation(recommendation)\n",
    "\n",
    "    # Create visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    visualize_annual_trends(recommendation, label_encoder)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  • gasf_image_Example_GASF_Image_Day_1.png\")\n",
    "    print(\"  • annual_energy_analysis.png\")\n",
    "\n",
    "    return recommendation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Run the complete pipeline\n",
    "    recommendation = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
